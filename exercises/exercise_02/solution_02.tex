\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{float}
\usepackage{listings}
\usepackage{color}
\usepackage{bbm}
\title{Reinforcement Learning \\ Exercise 1 - Solution}
\author{Jonathan Schnitzler}
\date{\today}
\begin{document}
\maketitle



%%%%%%%%%%%%%%%%%%%%%%%
%    Problem 1
%%%%%%%%%%%%%%%%%%%%%%%

\section{Formulating Problems}

\subsection*{a) The game of chess}

\paragraph*{States}
The position of all pieces on the board. A chess board is a 8x8 grid, which starts with 16 white and 16 black pieces on opposing sites. The state space is large (an upper bound from around $\approx 10^{45}$ \footnote{see \url{https://tromp.github.io/chess/chess.html}}).

\paragraph*{Actions}
The possible moves of the current player. The number of possible moves is limited by the number of pieces on the board and the rules of chess.

\paragraph*{Reward Signal}
\begin{itemize}
    \item win, lose or draw the game (by checkmate)
    \item evaluate the current position of the board (e.g. material advantage, positional advantage)
\end{itemize}




\subsection*{b) A pick and place robot}

\paragraph*{States}
\begin{itemize}
    \item position and orientation of the axes
    \item is holding something
    \item source of objects and destination
\end{itemize}

\paragraph*{Actions}
\begin{itemize}
    \item pick
    \item place
    \item repeat
\end{itemize}


\paragraph*{Reward Signal}
\begin{itemize}
    \item successfully pick and place an object
    \item time to pick and place an object
    \item lost an object
\end{itemize}




\subsection*{c) A drone which should stabelize in air}

\paragraph*{States}
\begin{itemize}
    \item tilt angle
\end{itemize}

\paragraph*{Actions}
\begin{itemize}
    \item adapt speed of individual rotors
\end{itemize}

\paragraph*{Reward Signal}
\begin{itemize}
    \item time in air
    \item minimize the tilt angle
    \item minimize steering (and energy consumption)
\end{itemize}


\subsection*{d) Playing tetris}

\paragraph*{States}
\begin{itemize}
    \item position of the falling block
    \item position of the other blocks
    \item preview of next block
\end{itemize}


\paragraph*{Actions}
\begin{itemize}
    \item move block left/right
    \item rotate block
\end{itemize}

\paragraph*{Reward Signal}
\begin{itemize}
    \item clear a row
    \item lose the game
\end{itemize}



\section{Value Functions}

\paragraph*{a) k-armed Bandits as MDP}

The Future rewards are independent on the current state. Since there is only one state, i.e. you are about to roll a bandit, the only thing which could change is our policy based on new estimates of the true distribution - exploration. 

This is not considered in the expected reward and therefore its basicly  the same $R_t = R$ for all rolls. Then it holds that the sum is just an additional factor, which is not relevant for the value function
\begin{equation}
    \sum_{k=0}^\infty \gamma^k R_{t+k+1} = R  \sum_{k=0}^\infty \gamma^k = R \frac{1}{1-\gamma}
\end{equation} 
since $\gamma < 1$.


\paragraph*{b) Proof Relation of value and action-value function}

We shall show that 
\begin{equation}
    v_\pi(s) = \sum_a \pi(a|s) q_\pi(s,a)
\end{equation}
holds. We can use the definition of the value function
\begin{equation}
    v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
\end{equation}
and the definition of the action-value function
\begin{equation}
    q_\pi(s,a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]
\end{equation}
to show the relation. We use the law of total expectation to condition on the action $A_t = a$:
\begin{align}
    v_\pi(s) &= \mathbb{E}_\pi[G_t | S_t = s] \\
       &= \sum_a p(A_t = a | S_t = s) \mathbb{E}_\pi[G_t | S_t = s, A_t = a] \cdot 
\end{align}


\paragraph*{c) Rephraze value function}



\section{Bruteforce the Policy Space}

\paragraph*{a) Number of policies}

There are $4\cdot 4 = 16$ tiles, which are the states. There are four actions, namely up, down, left and right. If the policy is deterministic, then for each tile the decision tree branches into four additional options, i.e.
\begin{equation}
    n_\pi = 4^{16} = 2^{32} \approx 4 \cdot 10^9
\end{equation}
which is doable for a computer but still kind of ridiculos.


\paragraph*{b) See Code}



\end{document}